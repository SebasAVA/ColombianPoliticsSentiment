{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d1707b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import language_tool_python\n",
    "import gensim\n",
    "from gensim import models\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "import tqdm \n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a36140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to preprocess the data\n",
    "def lemmatize(text,nlp):\n",
    "    # can be parallelized\n",
    "    doc = nlp(text)\n",
    "    lemma = [n.lemma_ for n in doc]\n",
    "    return lemma\n",
    "\n",
    "def preprocess(text,nlp):\n",
    "    \n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text): #  gensim.utils.simple_preprocess tokenizes el texto\n",
    "        token = ''.join(x for x in token.lower() if x.isalpha())\n",
    "        if token not in palabrasVacias_nltk and len(token) > 2:\n",
    "            result.append(token)       \n",
    "    result = lemmatize(' '.join(result),nlp)\n",
    "    return result\n",
    "\n",
    "def remove_words(text):\n",
    "    # Reemplazar simobolo por palabra para que no me elimine los hashtags\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b','',text, flags=re.MULTILINE) #Remove URL\n",
    "    text = re.sub(r'@\\w+','', text) # remove mentions\n",
    "    return text\n",
    "\n",
    "def correct_text(text):\n",
    "    coincidencias = corrector.check(text)\n",
    "    corrected = corrector.correct(text)\n",
    "    return corrected\n",
    "\n",
    "# load spanish language tool\n",
    "corrector = language_tool_python.LanguageTool('es')\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "# define stopwords\n",
    "palabrasVacias_nltk = stopwords.words('spanish')\n",
    "palabrasVacias_nltk.append(\"usted\")\n",
    "palabrasVacias_nltk.append(\"uds\")\n",
    "palabrasVacias_nltk.append(\"hacer\")\n",
    "palabrasVacias_nltk.append(\"bien\")\n",
    "palabrasVacias_nltk.append(\"navidad\")\n",
    "palabrasVacias_nltk.append(\"jajaja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24cdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fitted LDA\n",
    "lda_model = models.LdaModel.load(\"final_lda_model/final_lda_model.model\")\n",
    "\n",
    "# load the dictionary\n",
    "dictionary_spanish = gensim.corpora.Dictionary.load(\"final_lda_model/final_lda_model.model.id2word\")\n",
    "\n",
    "# define the tfidf from the beginning of january (which we used to tune the LDA)\n",
    "# read the old corpus and transform file\n",
    "with open('final_lda_model/bow_corpus.json') as f:\n",
    "   lda_corpus = [[tuple(i) for i in x] for x in json.load(f)]\n",
    "tfidf = models.TfidfModel(lda_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75fb76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tweet data and preprocess\n",
    "tweets = pd.read_csv('data/politicians_tweets.csv')\n",
    "tweets['tweet_c'] = tweets['label'].apply(lambda x: remove_words(x))\n",
    "preprocessed_tweets = tweets['tweet_c'].apply(lambda x: preprocess(x, nlp))\n",
    "\n",
    "# define the bag of word corpus for all current tweets\n",
    "bow_corpus = [dictionary_spanish.doc2bow(doc) for doc in preprocessed_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387ead3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1358/1358 [00:01<00:00, 1086.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# get topics for all tweets\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "tpcs = lda_model[tfidf_corpus]\n",
    "\n",
    "# extract the main topic and the corresponding score\n",
    "topic, score = [], []\n",
    "for t in tqdm.tqdm(tpcs):\n",
    "    topic.append(int(np.argmax([j[1] for j in t])))\n",
    "    score.append(float(np.max([j[1] for j in t])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "295c5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the topic names from Bibian\n",
    "topic_table = pd.read_excel(\"data/topic_names.xlsx\")\n",
    "topic_table[\"lda_topic\"] = topic_table.index\n",
    "topic_table[\"NAME\"] = [j.replace(\"\\n\",'') for j in topic_table['NAME']]\n",
    "topic_lda_dict = dict(zip(topic_table[\"lda_topic\"],topic_table[\"NUMBER OF TOPICS\"]))\n",
    "topic_label_dict = dict(zip(topic_table[\"NUMBER OF TOPICS\"],topic_table[\"NAME\"]))\n",
    "final_topic = [topic_lda_dict[j] for j in topic]\n",
    "final_label = [topic_label_dict[j] for j in final_topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2726d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of tuples\n",
    "def listOfTuples(l0,l1, l2):\n",
    "    return list(map(lambda w, x, y,:(w,x,y), l0,l1, l2))\n",
    "\n",
    "topic_list = listOfTuples(tweets['id'],final_topic,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "885996ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server connected\n",
      "database connected\n",
      "1358 Record inserted successfully into tweet_topic\n"
     ]
    }
   ],
   "source": [
    "# upload data to database\n",
    "\n",
    "import psycopg2\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "try:\n",
    "    with SSHTunnelForwarder(\n",
    "        ('161.35.123.231', 22),\n",
    "        ssh_username=\"postgres\",\n",
    "        ssh_password=\"dbConn2021!\",\n",
    "        remote_bind_address=('localhost', 5432)) as server:\n",
    "        \n",
    "        print(\"server connected\")\n",
    "        \n",
    "        keepalive_kwargs = {\n",
    "            \"keepalives\": 1,\n",
    "            \"keepalives_idle\": 60,\n",
    "            \"keepalives_interval\": 10,\n",
    "            \"keepalives_count\": 5\n",
    "        } \n",
    "        \n",
    "        params = {\n",
    "            'database': 'tweetproject',\n",
    "            'user': 'postgres',\n",
    "            'password': 'padova2021',\n",
    "            'host': server.local_bind_host,\n",
    "            'port': server.local_bind_port,\n",
    "            **keepalive_kwargs\n",
    "        }\n",
    "        \n",
    "        conn = psycopg2.connect(**params)\n",
    "        curs = conn.cursor()\n",
    "        print(\"database connected\")   \n",
    "        \n",
    "        sql = \"INSERT INTO tweet_topic(id_tweet,id_topic,score) VALUES(%s, %s, %s)\"\n",
    "        curs.executemany(sql,topic_list)\n",
    "        \n",
    "        conn.commit()\n",
    "        count = curs.rowcount\n",
    "        print(count, \"Record inserted successfully into tweet_topic\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Failed to insert record tweet_topic:\", error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
